{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataAggregator(path):\n",
    "    text_dict = {}\n",
    "    json_dict = {}\n",
    "    label_df = pd.DataFrame(columns=['changes', 'positions','file'])\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(path+file, 'r', encoding='utf-8') as myfile:\n",
    "                text_dict[file.replace('problem-', '').replace('.txt', '')] = myfile.read().replace('\\n', '')\n",
    "        else:\n",
    "            with open(path+file, 'r', encoding='utf-8') as myfile:\n",
    "                data = json.load(myfile)\n",
    "                json_dict[file.replace('problem-', '').replace('.truth', '')] = data['changes']\n",
    "\n",
    "    df = pd.DataFrame(list(text_dict.items()), columns=['file', 'text'])\n",
    "    label_df = pd.DataFrame(list(json_dict.items()), columns = ['file', 'changes'])\n",
    "\n",
    "    test_df = df.merge(label_df,on='file')\n",
    "    test_df = test_df.drop('file', axis = 1)\n",
    "    #Changed Column name for Weka compatibility\n",
    "    test_df.columns = ['df_text', 'df_changes']\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/saurabhnagrecha/Pandas-to-ARFF/blob/master/pandas2arff.py\n",
    "# Changed string replacement from \"_\" to \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pandas2arff(df,filename,wekaname = \"pandasdata\",cleanstringdata=True,cleannan=True):\n",
    "    \"\"\"\n",
    "    converts the pandas dataframe to a weka compatible file\n",
    "    df: dataframe in pandas format\n",
    "    filename: the filename you want the weka compatible file to be in\n",
    "    wekaname: the name you want to give to the weka dataset (this will be visible to you when you open it in Weka)\n",
    "    cleanstringdata: clean up data which may have spaces and replace with \"_\", special characters etc which seem to annoy Weka. \n",
    "                     To suppress this, set this to False\n",
    "    cleannan: replaces all nan values with \"?\" which is Weka's standard for missing values. \n",
    "              To suppress this, set this to False\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    \n",
    "    def cleanstring(s):\n",
    "\n",
    "        if s!=\"?\":\n",
    "            return re.sub('[^A-Za-z0-9]+', \" \", str(s)) #Derek note: replace \"_\" with \" \" \n",
    "        else:\n",
    "            return \"?\"\n",
    "\n",
    "\n",
    "            \n",
    "    dfcopy = df #all cleaning operations get done on this copy\n",
    "\n",
    "    \n",
    "    if cleannan!=False:\n",
    "        dfcopy = dfcopy.fillna(-999999999) #this is so that we can swap this out for \"?\"\n",
    "        #this makes sure that certain numerical columns with missing values don't get stuck with \"object\" type\n",
    " \n",
    "    f = open(filename,\"w\")\n",
    "    arffList = []\n",
    "    arffList.append(\"@relation \" + wekaname + \"\\n\")\n",
    "    #look at each column's dtype. If it's an \"object\", make it \"nominal\" under Weka for now (can be changed in source for dates.. etc)\n",
    "    for i in range(df.shape[1]):\n",
    "        if dfcopy.dtypes[i]=='O' or (df.columns[i] in [\"Class\",\"CLASS\",\"class\"]):\n",
    "            if cleannan!=False:\n",
    "                dfcopy.iloc[:,i] = dfcopy.iloc[:,i].replace(to_replace=-999999999, value=\"?\")\n",
    "            if cleanstringdata!=False:\n",
    "                dfcopy.iloc[:,i] = dfcopy.iloc[:,i].apply(cleanstring)\n",
    "            _uniqueNominalVals = [str(_i) for _i in np.unique(dfcopy.iloc[:,i])]\n",
    "            _uniqueNominalVals = \",\".join(_uniqueNominalVals)\n",
    "            _uniqueNominalVals = _uniqueNominalVals.replace(\"[\",\"\")\n",
    "            _uniqueNominalVals = _uniqueNominalVals.replace(\"]\",\"\")\n",
    "            _uniqueValuesString = \"{\" + _uniqueNominalVals +\"}\" \n",
    "            arffList.append(\"@attribute \" + df.columns[i] + _uniqueValuesString + \"\\n\")\n",
    "        else:\n",
    "            arffList.append(\"@attribute \" + df.columns[i] + \" real\\n\") \n",
    "            #even if it is an integer, let's just deal with it as a real number for now\n",
    "    arffList.append(\"@data\\n\")           \n",
    "    for i in range(dfcopy.shape[0]):#instances\n",
    "        _instanceString = \"\"\n",
    "        for j in range(df.shape[1]):#features\n",
    "                if dfcopy.dtypes[j]=='O':\n",
    "                    _instanceString+=\"\\\"\" + str(dfcopy.iloc[i,j]) + \"\\\"\"\n",
    "                else:\n",
    "                    _instanceString+=str(dfcopy.iloc[i,j])\n",
    "                if j!=dfcopy.shape[1]-1:#if it's not the last feature, add a comma\n",
    "                    _instanceString+=\",\"\n",
    "        _instanceString+=\"\\n\"\n",
    "        if cleannan!=False:\n",
    "            _instanceString = _instanceString.replace(\"-999999999.0\",\"?\") #for numeric missing values\n",
    "            _instanceString = _instanceString.replace(\"\\\"?\\\"\",\"?\") #for categorical missing values\n",
    "        arffList.append(_instanceString)\n",
    "    f.writelines(arffList)\n",
    "    f.close()\n",
    "    del dfcopy\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/home/derek/Downloads/Data/pan18train/'\n",
    "path_test = '/home/derek/Downloads/Data/pan18test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = DataAggregator(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = DataAggregator(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/derek/Downloads/Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas2arff(train_df, \"train_df.arff\", wekaname= \"pan18train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas2arff(test_df, \"test_df.arff\", wekaname= \"pan18test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
